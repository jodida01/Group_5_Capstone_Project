{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b1ecf8",
   "metadata": {},
   "source": [
    "# Project Overview: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd246c",
   "metadata": {},
   "source": [
    "\n",
    "## Collaborators:\n",
    "\n",
    "1. Agnes Chomba\n",
    "\n",
    "2. Derrick Malinga\n",
    "\n",
    "3. Erick Okacha\n",
    "\n",
    "4. Judah Odida\n",
    "\n",
    "5. Lucas Ominde\n",
    "\n",
    "6. Nick  Mwai\n",
    "\n",
    "7. Olgah Omollo\n",
    "\n",
    "# FinComBot - Compliance Chatbot \n",
    "\n",
    "## 1. Background\n",
    "Financial institutions face increasing pressure to comply with stringent regulatory frameworks governing customer onboarding, Know Your Customer (KYC), Customer Due Diligence (CDD), Enhanced Due Diligence (EDD), Anti-Money Laundering (AML), Counter Terrorism Financing, Counter Proliferation Financing (CPF), and sanctions screening. These obligations are complex, continuously evolving, and vary across jurisdictions.\n",
    "\n",
    "Staff often face difficulties accessing and interpreting regulatory documents and internal policies, leading to:\n",
    "-\tDelays in onboarding, affecting customer experience and revenue.\n",
    "-\tInconsistent application of compliance procedures.\n",
    "-\tOverdependence on compliance officers for basic guidance.\n",
    "-\tIncreased risk of regulatory breaches which may lead to fining by regulators and put the bank at risk of its license being suspended.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06af7f",
   "metadata": {},
   "source": [
    "#  2. Business Objective\n",
    "\n",
    "a.)  Build a chatbot that retrieves accurate compliance information \n",
    "from the bank‚Äôs KYC/AML/CTF/CPF policies and responds to staff queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d1ad0",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Target Audience\n",
    "\n",
    "a.) Front office / Relationship Managers (who onboard customers)\n",
    "\n",
    "b.)  Operations staff (who process documents)\n",
    "\n",
    "c.) Compliance officers (for guidance validation)\n",
    "\n",
    "d.) New staff (as a training tool)\n",
    "\n",
    "e.) Risk & Audit teams (for oversight)\n",
    "\n",
    "\n",
    "##  4. Data Understanding\n",
    "Data Source: \n",
    "a. Internal compliance policy, stored in Word (.docx) format,  Contains: KYC procedures, AML red flags, CDD/EDD checklists, risk rating methodology, regulatory guidelines (FATF, CBK, CMA)\n",
    "\n",
    "Data Characteristics:Unstructured text (paragraphs, checklists), Multiple sections (policies, procedures, workflows), Needs preprocessing before AI ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb0a67",
   "metadata": {},
   "source": [
    "### a. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8619ebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-10a14f605937>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdocx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docx'"
     ]
    }
   ],
   "source": [
    "\n",
    "import docx\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59635c",
   "metadata": {},
   "outputs": [],
   "source": [
    " -------------------------\n",
    "# 1. Load Word Document\n",
    "# -------------------------\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    return text\n",
    "\n",
    "# Example: replace with your compliance manual path\n",
    "file_path = \"compliance_manual.docx\"\n",
    "paragraphs = extract_text_from_docx(file_path)\n",
    "print(f\"Extracted {len(paragraphs)} paragraphs from document.\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Chunk the Text\n",
    "# -------------------------\n",
    "def chunk_text(paragraphs, chunk_size=300):\n",
    "    chunks, current = [], \"\"\n",
    "    for para in paragraphs:\n",
    "        if len(current) + len(para) < chunk_size:\n",
    "            current += \" \" + para\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = para\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(paragraphs)\n",
    "print(f\"Created {len(chunks)} chunks for embedding.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Visualizations: Data Understanding\n",
    "# -------------------------\n",
    "\n",
    "# Paragraph Length Distribution\n",
    "lengths = [len(p.split()) for p in paragraphs]\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(lengths, bins=20, kde=True)\n",
    "plt.title(\"Distribution of Paragraph Lengths (in words)\")\n",
    "plt.xlabel(\"Words per Paragraph\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Chunk Length Distribution\n",
    "chunk_lengths = [len(c.split()) for c in chunks]\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(chunk_lengths, bins=20, color=\"orange\", kde=True)\n",
    "plt.title(\"Distribution of Chunk Lengths (in words)\")\n",
    "plt.xlabel(\"Words per Chunk\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Top Keywords\n",
    "all_text = \" \".join(paragraphs).lower().split()\n",
    "common_words = [w for w in all_text if len(w) > 3]  # remove very short words\n",
    "word_freq = Counter(common_words).most_common(20)\n",
    "\n",
    "words, freqs = zip(*word_freq)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=list(freqs), y=list(words))\n",
    "plt.title(\"Top 20 Most Frequent Terms in Compliance Manual\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Keyword\")\n",
    "plt.show()\n",
    "\n",
    "# Word Cloud\n",
    "wc = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(common_words))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Compliance Manual\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# 4. Create Embeddings\n",
    "# -------------------------\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Store in FAISS\n",
    "# -------------------------\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Retrieval Function\n",
    "# -------------------------\n",
    "def search(query, top_k=3):\n",
    "    query_emb = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    results = [chunks[i] for i in indices[0]]\n",
    "    \n",
    "    # Visualization: Show similarity scores\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(x=list(range(1, top_k+1)), y=distances[0])\n",
    "    plt.title(f\"Retrieval Scores for Query: {query}\")\n",
    "    plt.xlabel(\"Top-k Retrieved Passages\")\n",
    "    plt.ylabel(\"Distance (Lower = More Similar)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return results, distances\n",
    "\n",
    "# -------------------------\n",
    "# 7. Generate Answer\n",
    "# -------------------------\n",
    "openai.api_key = \"YOUR_API_KEY\"  # replace with your key\n",
    "\n",
    "def generate_answer(query, retrieved_passages):\n",
    "    context = \"\\n\".join(retrieved_passages)\n",
    "    prompt = f\"\"\"\n",
    "    You are FinComBot, a compliance assistant.\n",
    "    Use ONLY the context below to answer. \n",
    "    If not found, say \"Not available in policies.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# -------------------------\n",
    "# 8. Example Query\n",
    "# -------------------------\n",
    "user_query = \"What documents are needed for Enhanced Due Diligence (EDD)?\"\n",
    "retrieved_passages, scores = search(user_query)\n",
    "final_answer = generate_answer(user_query, retrieved_passages)\n",
    "\n",
    "print(\"\\nüîç Query:\", user_query)\n",
    "print(\"\\nüìë Retrieved Context:\", retrieved_passages)\n",
    "print(\"\\nü§ñ FinComBot Answer:\", final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üìò FinComBot - Compliance Chatbot (MVP with Interactive Clustering)\n",
    "# ========================================\n",
    "\n",
    "# Requirements:\n",
    "# pip install python-docx sentence-transformers faiss-cpu openai matplotlib seaborn wordcloud scikit-learn plotly\n",
    "\n",
    "import docx\n",
    "import faiss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import openai\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load Word Document\n",
    "# -------------------------\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    return text\n",
    "\n",
    "file_path = \"compliance_manual.docx\"\n",
    "paragraphs = extract_text_from_docx(file_path)\n",
    "print(f\"Extracted {len(paragraphs)} paragraphs from document.\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Chunk the Text\n",
    "# -------------------------\n",
    "def chunk_text(paragraphs, chunk_size=300):\n",
    "    chunks, current = [], \"\"\n",
    "    for para in paragraphs:\n",
    "        if len(current) + len(para) < chunk_size:\n",
    "            current += \" \" + para\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = para\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(paragraphs)\n",
    "print(f\"Created {len(chunks)} chunks for embedding.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Create Embeddings\n",
    "# -------------------------\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Topic Clustering with t-SNE + Interactive Plotly\n",
    "# -------------------------\n",
    "print(\"Running t-SNE... this may take a few minutes for large documents.\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Assign topics by keyword\n",
    "def assign_topic(chunk):\n",
    "    text = chunk.lower()\n",
    "    if \"kyc\" in text:\n",
    "        return \"KYC\"\n",
    "    elif \"aml\" in text or \"anti-money laundering\" in text:\n",
    "        return \"AML\"\n",
    "    elif \"edd\" in text or \"enhanced due diligence\" in text:\n",
    "        return \"EDD\"\n",
    "    elif \"cdd\" in text or \"customer due diligence\" in text:\n",
    "        return \"CDD\"\n",
    "    elif \"risk\" in text:\n",
    "        return \"Risk\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "labels = [assign_topic(c) for c in chunks]\n",
    "\n",
    "# Build interactive scatter plot\n",
    "fig = px.scatter(\n",
    "    x=embeddings_2d[:,0],\n",
    "    y=embeddings_2d[:,1],\n",
    "    color=labels,\n",
    "    hover_data={\"Chunk\": chunks},\n",
    "    title=\"t-SNE Clustering of Compliance Manual (Interactive)\",\n",
    "    labels={\"x\": \"t-SNE Dimension 1\", \"y\": \"t-SNE Dimension 2\"}\n",
    ")\n",
    "fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "fig.show()\n",
    "\n",
    "# -------------------------\n",
    "# 5. Store in FAISS\n",
    "# -------------------------\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Retrieval Function\n",
    "# -------------------------\n",
    "def search(query, top_k=3):\n",
    "    query_emb = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    results = [chunks[i] for i in indices[0]]\n",
    "    \n",
    "    # Retrieval score visualization\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(x=list(range(1, top_k+1)), y=distances[0])\n",
    "    plt.title(f\"Retrieval Scores for Query: {query}\")\n",
    "    plt.xlabel(\"Top-k Retrieved Passages\")\n",
    "    plt.ylabel(\"Distance (Lower = Better)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return results, distances\n",
    "\n",
    "# -------------------------\n",
    "# 7. Generate Answer\n",
    "# -------------------------\n",
    "openai.api_key = \"YOUR_API_KEY\"  # replace with your key\n",
    "\n",
    "def generate_answer(query, retrieved_passages):\n",
    "    context = \"\\n\".join(retrieved_passages)\n",
    "    prompt = f\"\"\"\n",
    "    You are FinComBot, a compliance assistant.\n",
    "    Use ONLY the context below to answer. \n",
    "    If not found, say \"Not available in policies.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# -------------------------\n",
    "# 8. Example Query\n",
    "# -------------------------\n",
    "user_query = \"What documents are needed for Enhanced Due Diligence (EDD)?\"\n",
    "retrieved_passages, scores = search(user_query)\n",
    "final_answer = generate_answer(user_query, retrieved_passages)\n",
    "\n",
    "print(\"\\nüîç Query:\", user_query)\n",
    "print(\"\\nüìë Retrieved Context:\", retrieved_passages)\n",
    "print(\"\\nü§ñ FinComBot Answer:\", final_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
